[TOC]



### 前言

### 背景知识

#### 神经网络

神经网络最初的称呼是人工神经网络（Artificial Neural Network），因为当时的研究人员是出于模拟人类大脑的目的而开发出这种模型。先驱性的工作包括MP神经元和感知机（Perceptron）。1943年，McCullogh和Pitts在一篇名为“A logic calculus of the ideas immanent in nervous activity”的论文中，给出了一个高度精简的神经元模型，我们称之为MP神经元。它的主要工作流程是：接收多个信号输入w；计算加权和s=wx；如果s大于预设阈值theta，输出一个正信号，否则，什么都不输出（也可以认为是输出0）。通过选取恰当的接收权值w，我们可以使得神经元的输入输出关系和某些函数一致，比如逻辑或（OR）和逻辑与（AND）。MP神经元的出现非常鼓舞人心，似乎我们成功踏出了探索人脑的第一步，但是它不具备人脑那样的自动学习功能，因为权值w是需要预先计算好的，所以MP神经元实质上和一个数字逻辑电路没有区别。它的主要贡献是，首次提出了神经元这种基本的计算模型。基于该模型，Rosenblatt在1958年提出了一种具备学习功能的算法，其基本计算模型和MP神经元没有区别，不过是额外提出了一种自动求解权重的方法，合称为感知机。从机器学习的角度看，感知机得到的其实是一个线性二分类器，假设数据集是线性可分的，学习算法保证了一定可以在有限次更新后找到一个将两类完美分开的超平面。从下图可以看出，OR和AND运算都可以用感知机模拟，但是XOR就不行了。

![OR、AND和XOR的分界面](http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif)

##### 前馈神经网络

感知机计算出的是一个在输入空间的线性分类函数，但在实际应用中，分界线几乎不可能是线性的。不过，在其他空间中，这些点可能存在线性分界线。这提示我们添加额外的转换过程。下图展示了如何通过不断的折叠，使得非线性分界线越来越接近线性。折叠操作可以通过绝对值函数实现，折叠的轴线通过一个感知机定义。

<img src="https://cdn-images-1.medium.com/max/1600/1*RRuaYJIdEH8E3bL9oySR7A.png" height="150px"/>

这样的充当转换功能的层被称为隐含层，因为它对应的数据即不是输入也不是输出。包含至少一个隐含层的网络被称为前馈神经网络，在这个网络中最下面的层为输入层，该层的神经元被称为输入神经元。



<img src="https://cdn-images-1.medium.com/max/1200/1*p9DcqNlVia4qBc8wcwD1Ow.png" height="300px"/>

wl(i,j)是第l层的第j神经元和l+1层的i神经元的连接权值，bi表示偏置项，h(x)是激活函数，fx是整个网络表示的从x到y的映射。值得注意的一点是，包含至少一个隐含层的前馈神经网络具备全局近似能力，1989年Horniket等人证明了使用足够多的隐含层节点，网络可以以任意精度逼近任意一个函数。这告诉我们神经网络模型可以用于拟合任何函数，但是这个理论在实际应用中并没有什么指导价值，因为它没有给出多少个节点数量才是足够的。

基本组成单元

基本结构

拟合能力、可训练性

比较成功的应用

局限性->深度学习 hinton文章，多隐含层的表达能力

##### 后向传播

#### 卷积神经网络

##### CNN的结构

##### 训练CNN

#### 过拟合与欠拟合

### 显式正则化

#### 参数衰减

#### 噪声引入

##### 随机梯度下降

##### Dropout

##### Batch Normalization

### 数据扩增

#### 仿射变换

#### 图像增强

### 特征扩增

#### 特征扩增层

#### 扩增操作

#### 实验

#### 结论

### 组合实验

### 总结



#### 



