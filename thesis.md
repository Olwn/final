### 前言

### 背景知识

#### 神经网络

神经网络最初的称呼是人工神经网络（Artificial Neural Network），因为当时的研究人员是出于模拟人类大脑的目的而开发出这种模型。先驱性的工作包括MP神经元和感知机（Perceptron）。1943年，McCullogh和Pitts在一篇名为“A logic calculus of the ideas immanent in nervous activity”的论文中，给出了一个高度精简的神经元模型，我们称之为MP神经元。它的主要工作流程是：接收多个信号输入w；计算加权和s=wx；如果s大于预设阈值theta，输出一个正信号，否则，什么都不输出（也可以认为是输出0）。通过选取恰当的接收权值w，我们可以使得神经元的输入输出关系和某些函数一致，比如逻辑或（OR）和逻辑与（AND）。MP神经元的出现非常鼓舞人心，似乎我们成功踏出了探索人脑的第一步，但是它不具备人脑那样的自动学习功能，因为权值w是需要预先计算好的，所以MP神经元实质上和一个数字逻辑电路没有区别。它的主要贡献是，首次提出了神经元这种基本的计算模型。基于该模型，Rosenblatt在1958年提出了一种具备学习功能的算法，其基本计算模型和MP神经元没有区别，不过是额外提出了一种自动求解权重的方法，合称为感知机。从机器学习的角度看，感知机得到的其实是一个线性二分类器，假设数据集是线性可分的，学习算法保证了一定可以在有限次更新后找到一个将两类完美分开的超平面。从下图可以看出，OR和AND运算都可以用感知机模拟，但是XOR就不行了。

![OR、AND和XOR的分界面](http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif)

##### 前馈神经网络

感知机计算出的是一个在输入空间的线性分类函数，但在实际应用中，分界线几乎不可能是线性的。不过，在其他空间中，这些点可能存在线性分界线。这提示我们添加额外的转换过程。下图展示了如何通过不断的折叠，使得非线性分界线越来越接近线性。折叠操作可以通过绝对值函数实现，折叠的轴线通过一个感知机定义。

<img src="https://cdn-images-1.medium.com/max/1600/1*RRuaYJIdEH8E3bL9oySR7A.png" height="150px"/>

这样的充当转换功能的层被称为隐含层，因为它对应的数据即不是输入也不是输出。前馈神经网络包含至少一个隐含层，更一般的定义是多个神经预元节点连接形成的一个有向无环图，但是通常这些节点会被组织为一层一层的，整个模型是一个简单、统一的层叠式网络，如图所示。典型的前馈神经网络是全连接的，每个节点会和前后相邻的两层的所有节点相连，但是层内的节点没有连接。隐含层的层数和每层节点数是可以自由配置的，网络的拟合能力是和这两个量正相关的。找到一个好的配置并不容易，太多节点会导致网络过于完美的拟合训练集，太少节点的网络不足以表达数据的分布。包含一个隐含层的前馈神经网络具备全局近似能力，1989年Horniket等人证明了使用足够多的隐含层节点，网络可以以任意精度逼近任意一个函数。这告诉我们神经网络模型可以用于解决任何映射学习问题，但是这个理论在实际应用中并没有什么指导价值，因为它只给出了一个非常大的上限，而且没有给出下限。对于一个具体的任务，我们仍然无法知道多少个节点数量是合适的。



<img src="http://neuralnetworksanddeeplearning.com/images/tikz41.png" height="300px"/>

前馈神经网络可以被看成一个通用的回归或分类模型。和其他机器学习模型的求解方法一样，我们需要先定义一个损失函数，该函数的值代表了网络目前的行为和数据集的接近程度。训练的过程是调整网络的连接权值，使得损失函数在训练集上的均值尽量小。使用最广泛的优化方法是梯度下降法，该算法收敛到全局最优的条件是优化对象是一个凸函数。尽管多层神经网络的损失函数不具备这样的性质，但在实际使用中，梯度下降法几乎总能得到不错的结果。这一点可能和高维度的特性相关。



基本组成单元 基本结构 拟合能力、可训练性 比较成功的应用 hinton文章，多隐含层的表达能力

##### 后向传播

梯度下降法的基本思路是求出目标函数对自变量（权值）的梯度，然后向梯度的反方向更新权值。后向传播（BackPropogation，简称BP）算法是一种高效的梯度计算方法，由Werbos在19xx年首次提出，后来Hinton等人再次独立提出。之所以出现同一个研究成果被多次提出，是因为当年正处于“AI寒冬”，神经网络会议稀少，研究人员之间缺乏交流。这种算法的基本思想是利用后一层节点的梯度计算当前层的权值的梯度，这样计算整个网络的参数的梯度的复杂度是O（N+M），N是节点数量，M是连接数量。BP算法在神经网络领域享有很高的荣誉，很多人认为它的出现使得多层神经网络的训练才得以解决，然而笔者认为它不过是复合函数求导中的级联法则的一个简单应用。完全抛开BP算法，我们照样可以求出参数的梯度，只是效率会低一些。它解决的只是训练得快不快的问题，而不是能不能训练的问题。因此，可以认为BP算法被严重过誉了。

#### 卷积神经网络

相同：卷积神经网络和前馈神经网络非常相似，都是由一些包含可学习的权值的神经元构成。每个神经元从前一层接收输入，执行点积运算，然后通过一个非线性激活函数转换特征空间。整个网络仍然表达了一个函数，自变量是图像像素，因变量是某种高层信息的分数，比如图像属于某些类别的概率。

![NN vs CNN](http://cs231n.github.io/assets/cnn/cnn.jpeg)

不同：不过，卷积神经网络是专为图像数据设计的，其层与层的连接方式有变化，训练方式也有一定变动。前馈神经网络的每个神经元的输入包括了前一层的所有单元，这种连接方式并不适用于图像数据。虽然图像有确切的尺寸，维度是固定的，但它并不是严格意义上的结构化数据，因为某个维度表达的语义并不明确。图像的特点是局部相关，任意截取一个小块，都可以归纳出某种模式，比如图像左右两部分的颜色差异较大的是一个边界，中间和两边差异较大的是一条线段，这种可归纳性在图像的每个局部都成立。而图像整体并不具备这样的特性，因为图像生成的机制实在是太复杂了。卷积神经网络可被看成一种递归滤波。像素的简单组合是线条，线条的简单组合是纹理，而纹理的简单组合就可能是某种物体了。卷积神经网络的每一层的神经元构成一个三维结构，由宽度、高度和通道数定义。每个神经元只和前一层的某个区域的神经元相连接，同层的不同节点和前一层的连接权重是共享的，这和全连接有明显的区别。

##### CNN的结构

典型的CNN由卷积层、Pooling层和全连接层组成。

卷积层由多个可学习的滤波器（filter）组成。滤波器是三维的，其长度和宽度都比较小，一般限制在10以内，通道数和输入保持一致。卷积其实是一个滑动的过程，滤波器在输入特征的横向和纵向方向上移动，每个位置上单独计算点积，得到的是一个二维的特征图。特征图的值代表了输入特征的各个局部和滤波器的相似程度。卷积层由三个主要的超参数定义：滤波器数量、滤波器尺寸和滑动步长。滤波器的数量会显著影响卷积层的表达力，因为每个滤波器学习到的都是独立的模式，更多的滤波器可以提取到更丰富的信息。比如作用于原始图像的第一层卷积层，需要捕捉不同方向的线条、不同颜色的色块等。一个卷积层通常包含了几十到几百个滤波器，每个独立产生一个二维特征图。这些特征图堆叠起来就是卷积层的输出。滤波器的尺寸也会影响卷积层的表达力和稳定性。大卷积核能够提取到复杂的模式，但是也可能捕捉到噪音；小卷积核提取的是小范围的简单模式。将多个小卷积核级联可以获得更大的视野范围，比如两层3*3的卷积核的表达力等价于一层5、5的。实验表明，降低卷积核的尺寸并增加网络的层数可以获得泛化能力更好的网络。滑动步长决定了输出的特征图是否会被降采样。可以单独设置横向和纵向的步长，不过一般两个值相等，以s表示。若s为1，那么输出特征图的尺寸几乎等于输入特征图。若s大于1，那么输出特征图缩小为原来的1/s左右。

Pooling层的作用是去除冗余信息。它把相邻的多个响应值合并为一个，因此它也降低了特征图的尺寸。通常，每一定数量的卷积层后面会被添加一个Pooling层，越到网络的高层，特征图的尺寸越低。这使得位于高层的小卷积核也获得了大的视野范围。根据合并方式的不同，可以分为Max-Pooling和Avg-Pooling，使用最广泛的是前者。另一个关键参数是窗口尺寸，一般是2，2。Pooling层中的滑动步长一般等于窗口尺寸，意味着滑动时不会出现重叠。Pooling层不是必需的，将卷积层的滑动步长设置为大于1的值同样可以降低特征图的尺寸。

CNN中的全连接层和前馈神经网络中的完全一样，此处不再赘述。有实验表明，利用最后一个卷积层输出的特征图训练一个SVM分类器，可以达到类似的识别精度。这说明CNN的卷积层部分是一个特征提取器，而最后的全连接层充当分类器。

##### 训练CNN

梯度求解

参数初始化

##### 常见模型

LeNet是由LeCun等人在90年代开发的用于手写数字识别的网络模型，识别率达到99%以上。它的主要结构是两层5，5卷积层加上两次2，2降采样。这是卷积神经网络的第一个成功应用，它显示CNN这种简洁的模型在处理图像识别上的巨大潜力。这可能激励了人们尝试开发更大规模的CNN以处理更复杂的物体识别。

2012年ImageNet图像识别比赛的获胜模型AlexNet证明了这是可以做到的，它实现了84%的top5识别率，远远超过使用传统方法的第二名获得的74%识别率。这项工作由Alex Krizhevsky等人完成，网络结构和LeNet类似，但是层数和卷积核数量都更多，总共有5个卷积层和三次降采样。它表明了CNN在处理复杂的图像识别问题上的有效性，促成了神经网络的再次流行，许多计算机视觉研究人员自此开始研究CNN。

2014年ImageNet的获胜模型VGGNet继续大幅度提升了识别性能，top5识别率达到了93%左右。它包含了13个卷积层和5次降采样。和AlexNet的明显区别是卷积层的数量增加了一倍多，它显示了CNN的层数对模型效果的决定性作用。然而，随着层数的增加，网络的训练越来越成为问题。

![近几年ImageNet比赛的获奖模型和识别率](https://www.bdti.com/sites/default/files/insidedsp/articlepix/201706/MicrosoftImagNetResults.png)

2015年何凯明等人在改进了网络结构并结合了其他人的多项工作后，训练出了超过100层的网络模型，ResNet。这也是当年ImageNet的获胜模型，top5识别率提升到96%左右。ResNet中引入的隔层连接显著改善了神经网络的训练难的问题，到目前（2017年末）ResNet仍然是CNN研究和应用的首选基准模型。本文的后续实验也都基于该模型。

#### 图像分类问题

图像分类是给一副图像分配一个或多个标签，指出图像描绘了什么场景、包含了什么物体等。只有进行了这样的转换，计算机才可以一定程度上理解图像，否则它只是一个没有任何语义信息的矩阵。图像分类是计算机视觉的核心问题，因为计算机视觉领域的大多数其他问题都可以被分解为分类问题，比如物体识别是在图像的多个位置进行分类，图像分割是在像素级别上分类。然而，这个基础问题又非常困难。图像的生成过程受很多因素影响，可以将其分为环境条件和自身变化两类。前者包括光照、拍摄角度和遮挡，后者的情况更加复杂，以人脸为例，个体的发型、饰物和表情变化都将显著改变图像矩阵的取值。这些因素的自由度都太大，对其正向建模几乎不可能。因此，以数据驱动位思想的机器学习方法更合适。卷积神经网络就是为分析图像而设计的一种机器学习模型，近些年成为图像分类的标准方法。本文也以在图像分类问题上的效果来衡量正则化方法对CNN的有效性。

##### 数据集

MNIST

Fashion-MNIST

CIFAR-10是一个很流行的图像分类测试数据集。它包含的是3通道的32，32尺寸的图像，类别包含了猫、狗等动物和轿车、轮船等交通工具共十个。图像总量是6万张，被划分为5万和1万的两个子集，一般分别被当做训练集和测试集。另有CIFAR-100数据集，包含的图像和CIFAR-10完全一致，不过类别的粒度更细，比如人被分为了婴儿、男孩和女孩等，总计有100个类别。

ImageNet

#### 过拟合与正则化

##### VC维

##### bias-variance困境

### 显式正则化

训练神经网络时，我们是根据损失函数在训练集上的取值来调整参数的，这可以确保模型在训练集上取得好的效果，但是模型的价值是在测试集上体现的。神经网络作为一种通用的函数拟合手段，其被应用在特定问题时过拟合很容易发生。正则化就是通过设计一些策略，降低模型在测试集上的错误率（这可能会带来训练集误差的上升）。目前有很多针对卷积神经网络设计的正则化方法，可以归结到三类：作用于模型上的、作用于训练方法上的和作用于数据上的。作用于模型上的正则化方法是在网络结构上增加额外的层，训练和测试时这些层都和正常的节点层一样参与计算，代表性的有Dropout和Batch-Normalization；作用于训练方法上的正则化方法一般是修改梯度求解公式，比如在目标函数上添加一个额外项，它们只是训练时发挥作用，代表性方法有SGD和参数衰减；作用于数据上的正则化方法是修改数据集或标签，比如数据扩增和标签平滑。

#### 参数衰减

最常见的显示正则化方法是在损失函数 $J(w)$上添加权值的范式$O(w)$，我们用$J'(w)$来表示正则化后的损失函数$J'$
$$
J'(\theta) = J(\theta) + \lambda*O(\theta)
$$
$O(\theta)$和$\theta$的绝对值正相关，将其添加到损失函数，可以使得训练时同时降低原始损失函数和参数的范式。它限制了参数的变动空间，促使其向更小的绝对值、更多的零取值靠拢，从而限制了网络表示的函数的复杂度。 a是一个超参数，该值越大，正则化的作用越明显，不过太大的a值会干扰原始损失函数的下降。在实验中，可以根据$J(t)$和$O(t)$的数量级，选择适当的$\lambda$使得$\lambda*O(t)$的数量级和$J(t)$相当或者略小。$O(t)$仅定义在网络的连接权重上，而不包括偏置项，因为后者只充当常数项，不影响网络的复杂度。另外从训练时对数据的需求来看，连接权重更容易和过拟合相关，因为它影响的是两个节点的交互，需要观察到两个节点的各种取值的组合，而偏置只控制一个单一的节点。下文中，我们用$w$表示和正则相关的参数，$t$表示所有参数（包含$w$）。

以上是定性的分析，下面定量的分析$l2$正则是如何影响权值的更新方式和收敛的极值点的。$l2$正则又称为岭回归、Tikhonov正则，它通过添加$O(t) = \left\|w\right\|_{2}$到损失函数来实现。为了简化公式，且在不影响分析的前提下，我们假设没有偏置项，那么$\theta$就是$w$，那么

$$
J'(w) = \frac{1}{2} \lambda*\left\|w\right\|_{2} + J(w)
$$
其在$w$上的梯度是

$$
\nabla _w J = \lambda w + \nabla _w J
$$
按梯度下降法的更新公式，一次迭代是
$$
w = w - \epsilon (\lambda w + \nabla _w J) = (1-\epsilon \lambda)w - \epsilon \nabla _w J
$$
可以看到添加的正则项改变了梯度规则，其相当于每次先缩小w，再执行和原来一样的更新。这是单次更新的情况，我们需要知道正则项对w最后的收敛值有什么影响。梯度下降法的收敛于梯度为0的地方，即驻点。令$w^*=arg min J(w)$为没有正则化的收敛点，我们用二阶泰勒展开式近似$J(w)$在$w^*$附近的取值。
$$
J(w) = J(w^{*}) + 1/2(w-w^{*})^{T}H(w-w^{*})
$$
其中，$H$是$J$在$w^*$处的黑塞矩阵。根据驻点的定义，可以得到
$$
H(w-w^{*}) = 0
$$
设$w1$为有$l2$正则项的损失函数的驻点，有
$$
H(w-w1) + a*w = 0 => w1 = {(H + aI)}^{-1}Hw^{*}
$$

因为黑塞矩阵H是实对称的，可以被分解为$H=Q\Lambda Q^T$，$Q$是正交矩阵。应用到上面的式子，有
$$
w1=Q(\Lambda  + \alpha I)^{-1}Q^T w^*
$$
可以看出$l2$正则使得$w^*$按照$H$的特征向量定义的方向缩放，各个方向缩放的比例是$s=\frac {\lambda _i}{\lambda _i + \alpha}$。当$\lambda _i$靠近0时，$s$取得很小的值，对应的权重$w_i$会显著变小；当$\lambda _i$比较大时，s接近1，对应的权重$w _i$基本保持不变。而特征值代表了对应方向上的特征对目标函数的影响能力，l2正则进一步使得不重要的特征的权值减小，而重要的特征的权值得以保留。另外一种通用的权值衰减项是$l1$正则，它把$\Omega(\theta)= \left\|w\right\|_1$加到损失函数上。通过和上面类似的推导步骤可以知道，$l1$范式可以使得某些连接权重为0，这和稀疏编码的思想有些相似。下图形象地展示了$l1$和$l2$正则是如何改变损失函数的极值点的。

![L1 & L2 weight decay](http://hpzhao.com/images/L2_4.png)

#### 随机梯度下降

神经网络的训练是一个非凸优化问题，可以被表示为
$$
min J(w) = \frac{1}{|M|} \Sigma J_i(w)
$$
M代表整个数据集，$J_i(w)$是损失函数在第$i$条数据上的取值。按照梯度下降的定义，应当求解$J$在所有数据上的平均梯度，但实际上几乎没有人这么做，而是随机的从数据集中选取一个很小的子集计算。基于这种梯度计算方式的梯度下降法被称为随机梯度下降法（SGD），其已经成为深度学习社区的标准优化方法。Bottou等人证明了它在非凸问题上可以收敛到平稳点，Ge等人的工作表明SGD可以帮助逃离非凸函数上广泛分布的鞍点。一次更新的公式如下
$$
w_{t+1}=w_t-\epsilon(\frac{1}{|B|} \Sigma {\nabla J_i(w)})
$$
B代表从M中随机抽取出的固定大小的子集，满足$|B|<<|M|$，通常$|B|\in {\{32,64,…,512}\}$。这种修改可以带来两个好处。第一，它使得梯度计算的时间复杂度不再和数据集大小成正比，而是一个常数。在百万级的数据集上，SGD相比原始的GD算法有上万倍的加速。第二，它为训练出的网络模型带来了更好的泛化能力。SGD训练出的网络比GD泛化性能好，基于较小batch-size（几十）的SGD相比较大batch-size（几百）的SGD，泛化性能还有进一步提升。Nitish等人研究了使用不同的batch-size训练的收敛点附近的性质，发现batch-size和损失函数在收敛点附近的黑塞矩阵的特征值的绝对值成负相关。他们的实验证实了，基于小batch-size的SGD因梯度估计偏差而引入的噪声非但没有干扰网络收敛，而且帮助网络逃离了陡峭的极值点而收敛于平坦的区域。

![平坦和陡峭的极值点](http://www.inference.vc/content/images/2017/05/Screen-Shot-2017-05-25-at-1.36.40-PM.png)

平坦的极值点意味着函数在这附近较大区域内的变化率都很小，而陡峭的极值点表示附近很小区域内的变化率就比较大。大的变化率会对泛化能力造成负面影响，这可以通过信息论中的最小描述长度（MDL）理论解释，MDL值越小的统计模型具有更低的复杂度，泛化能力可能也更好。平坦区域的MDL比陡峭区域小，因此是更好的收敛点。

#### Dropout

#### Batch Normalization

#### 数据扩增

### 正则化对模型识别率的影响

### 正则化对模型稳定性、sharpness的影响

### 一种新的正则化方法：特征扩增

#### 定义

#### 实验

#### 分析

### 总结

