### 前言

### 背景知识

#### 神经网络

神经网络最初的称呼是人工神经网络（Artificial Neural Network），因为当时的研究人员是出于模拟人类大脑的目的而开发出这种模型。先驱性的工作包括MP神经元和感知机（Perceptron）。1943年，McCullogh和Pitts在一篇名为“A logic calculus of the ideas immanent in nervous activity”的论文中，给出了一个高度精简的神经元模型，我们称之为MP神经元。它的主要工作流程是：接收多个信号输入w；计算加权和s=wx；如果s大于预设阈值theta，输出一个正信号，否则，什么都不输出（也可以认为是输出0）。通过选取恰当的接收权值w，我们可以使得神经元的输入输出关系和某些函数一致，比如逻辑或（OR）和逻辑与（AND）。MP神经元的出现非常鼓舞人心，似乎我们成功踏出了探索人脑的第一步，但是它不具备人脑那样的自动学习功能，因为权值w是需要预先计算好的，所以MP神经元实质上和一个数字逻辑电路没有区别。它的主要贡献是，首次提出了神经元这种基本的计算模型。基于该模型，Rosenblatt在1958年提出了一种具备学习功能的算法，其基本计算模型和MP神经元没有区别，不过是额外提出了一种自动求解权重的方法，合称为感知机。从机器学习的角度看，感知机得到的其实是一个线性二分类器，假设数据集是线性可分的，学习算法保证了一定可以在有限次更新后找到一个将两类完美分开的超平面。从下图可以看出，OR和AND运算都可以用感知机模拟，但是XOR就不行了。

![OR、AND和XOR的分界面](http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif)

##### 前馈神经网络

感知机计算出的是一个在输入空间的线性分类函数，但在实际应用中，分界线几乎不可能是线性的。不过，在其他空间中，这些点可能存在线性分界线。这提示我们添加额外的转换过程。下图展示了如何通过不断的折叠，使得非线性分界线越来越接近线性。折叠操作可以通过绝对值函数实现，折叠的轴线通过一个感知机定义。

<img src="https://cdn-images-1.medium.com/max/1600/1*RRuaYJIdEH8E3bL9oySR7A.png" height="150px"/>

这样的充当转换功能的层被称为隐含层，因为它对应的数据即不是输入也不是输出。前馈神经网络包含至少一个隐含层，更一般的定义是多个神经预元节点连接形成的一个有向无环图，但是通常这些节点会被组织为一层一层的，整个模型是一个简单、统一的层叠式网络，如图所示。典型的前馈神经网络是全连接的，每个节点会和前后相邻的两层的所有节点相连，但是层内的节点没有连接。隐含层的层数和每层节点数是可以自由配置的，网络的拟合能力是和这两个量正相关的。找到一个好的配置并不容易，太多节点会导致网络过于完美的拟合训练集，太少节点的网络不足以表达数据的分布。包含一个隐含层的前馈神经网络具备全局近似能力，1989年Horniket等人证明了使用足够多的隐含层节点，网络可以以任意精度逼近任意一个函数。这告诉我们神经网络模型可以用于解决任何映射学习问题，但是这个理论在实际应用中并没有什么指导价值，因为它只给出了一个非常大的上限，而且没有给出下限。对于一个具体的任务，我们仍然无法知道多少个节点数量是合适的。



<img src="http://neuralnetworksanddeeplearning.com/images/tikz41.png" height="300px"/>

前馈神经网络可以被看成一个通用的回归或分类模型。和其他机器学习模型的求解方法一样，我们需要先定义一个损失函数，该函数的值代表了网络目前的行为和数据集的接近程度。训练的过程是调整网络的连接权值，使得损失函数在训练集上的均值尽量小。使用最广泛的优化方法是梯度下降法，该算法收敛到全局最优的条件是优化对象是一个凸函数。尽管多层神经网络的损失函数不具备这样的性质，但在实际使用中，梯度下降法几乎总能得到不错的结果。这一点可能和高维度的特性相关。



基本组成单元 基本结构 拟合能力、可训练性 比较成功的应用 hinton文章，多隐含层的表达能力

##### 后向传播

梯度下降法的基本思路是求出目标函数对自变量（权值）的梯度，然后向梯度的反方向更新权值。后向传播（BackPropogation，简称BP）算法是一种高效的梯度计算方法，由Werbos在19xx年首次提出，后来Hinton等人再次独立提出。之所以出现同一个研究成果被多次提出，是因为当年正处于“AI寒冬”，神经网络会议稀少，研究人员之间缺乏交流。这种算法的基本思想是利用后一层节点的梯度计算当前层的权值的梯度，这样计算整个网络的参数的梯度的复杂度是O（N+M），N是节点数量，M是连接数量。BP算法在神经网络领域享有很高的荣誉，很多人认为它的出现使得多层神经网络的训练才得以解决，然而笔者认为它不过是复合函数求导中的级联法则的一个简单应用。完全抛开BP算法，我们照样可以求出参数的梯度，只是效率会低一些。它解决的只是训练得快不快的问题，而不是能不能训练的问题。因此，可以认为BP算法被严重过誉了。

#### 卷积神经网络

相同：卷积神经网络和前馈神经网络非常相似，都是由一些包含可学习的权值的神经元构成。每个神经元从前一层接收输入，执行点积运算，然后通过一个非线性激活函数转换特征空间。整个网络仍然表达了一个函数，自变量是图像像素，因变量是某种高层信息的分数，比如图像属于某些类别的概率。

![NN vs CNN](http://cs231n.github.io/assets/cnn/cnn.jpeg)

不同：不过，卷积神经网络是专为图像数据设计的，其层与层的连接方式有变化，训练方式也有一定变动。前馈神经网络的每个神经元的输入包括了前一层的所有单元，这种连接方式并不适用于图像数据。虽然图像有确切的尺寸，维度是固定的，但它并不是严格意义上的结构化数据，因为某个维度表达的语义并不明确。图像的特点是局部相关，任意截取一个小块，都可以归纳出某种模式，比如图像左右两部分的颜色差异较大的是一个边界，中间和两边差异较大的是一条线段，这种可归纳性在图像的每个局部都成立。而图像整体并不具备这样的特性，因为图像生成的机制实在是太复杂了。卷积神经网络可被看成一种递归滤波。像素的简单组合是线条，线条的简单组合是纹理，而纹理的简单组合就可能是某种物体了。卷积神经网络的每一层的神经元构成一个三维结构，由宽度、高度和通道数定义。每个神经元只和前一层的某个区域的神经元相连接，同层的不同节点和前一层的连接权重是共享的，这和全连接有明显的区别。

##### CNN的结构

典型的CNN由卷积层、Pooling层和全连接层组成。

卷积层由多个可学习的滤波器（filter）组成。滤波器是三维的，其长度和宽度都比较小，一般限制在10以内，通道数和输入保持一致。卷积其实是一个滑动的过程，滤波器在输入特征的横向和纵向方向上移动，每个位置上单独计算点积，得到的是一个二维的特征图。特征图的值代表了输入特征的各个局部和滤波器的相似程度。卷积层由三个主要的超参数定义：滤波器数量、滤波器尺寸和滑动步长。滤波器的数量会显著影响卷积层的表达力，因为每个滤波器学习到的都是独立的模式，更多的滤波器可以提取到更丰富的信息。比如作用于原始图像的第一层卷积层，需要捕捉不同方向的线条、不同颜色的色块等。一个卷积层通常包含了几十到几百个滤波器，每个独立产生一个二维特征图。这些特征图堆叠起来就是卷积层的输出。滤波器的尺寸也会影响卷积层的表达力和稳定性。大卷积核能够提取到复杂的模式，但是也可能捕捉到噪音；小卷积核提取的是小范围的简单模式。将多个小卷积核级联可以获得更大的视野范围，比如两层3*3的卷积核的表达力等价于一层5、5的。实验表明，降低卷积核的尺寸并增加网络的层数可以获得泛化能力更好的网络。滑动步长决定了输出的特征图是否会被降采样。可以单独设置横向和纵向的步长，不过一般两个值相等，以s表示。若s为1，那么输出特征图的尺寸几乎等于输入特征图。若s大于1，那么输出特征图缩小为原来的1/s左右。

Pooling层的作用是去除冗余信息。它把相邻的多个响应值合并为一个，因此它也降低了特征图的尺寸。通常，每一定数量的卷积层后面会被添加一个Pooling层，越到网络的高层，特征图的尺寸越低。这使得位于高层的小卷积核也获得了大的视野范围。根据合并方式的不同，可以分为Max-Pooling和Avg-Pooling，使用最广泛的是前者。另一个关键参数是窗口尺寸，一般是2，2。Pooling层中的滑动步长一般等于窗口尺寸，意味着滑动时不会出现重叠。Pooling层不是必需的，将卷积层的滑动步长设置为大于1的值同样可以降低特征图的尺寸。

CNN中的全连接层和前馈神经网络中的完全一样，此处不再赘述。有实验表明，利用最后一个卷积层输出的特征图训练一个SVM分类器，可以达到类似的识别精度。这说明CNN的卷积层部分是一个特征提取器，而最后的全连接层充当分类器。

##### 训练CNN

梯度求解

参数初始化

##### 常见模型

LeNet是由LeCun等人在90年代开发的用于手写数字识别的网络模型，识别率达到99%以上。它的主要结构是两层5，5卷积层加上两次2，2降采样。这是卷积神经网络的第一个成功应用，它显示CNN这种简洁的模型在处理图像识别上的巨大潜力。这可能激励了人们尝试开发更大规模的CNN以处理更复杂的物体识别。

2012年ImageNet图像识别比赛的获胜模型AlexNet证明了这是可以做到的，它实现了84%的top5识别率，远远超过使用传统方法的第二名获得的74%识别率。这项工作由Alex Krizhevsky等人完成，网络结构和LeNet类似，但是层数和卷积核数量都更多，总共有5个卷积层和三次降采样。它表明了CNN在处理复杂的图像识别问题上的有效性，促成了神经网络的再次流行，许多计算机视觉研究人员自此开始研究CNN。

2014年ImageNet的获胜模型VGGNet继续大幅度提升了识别性能，top5识别率达到了93%左右。它包含了13个卷积层和5次降采样。和AlexNet的明显区别是卷积层的数量增加了一倍多，它显示了CNN的层数对模型效果的决定性作用。然而，随着层数的增加，网络的训练越来越成为问题。

![近几年ImageNet比赛的获奖模型和识别率](https://www.bdti.com/sites/default/files/insidedsp/articlepix/201706/MicrosoftImagNetResults.png)

2015年何凯明等人在改进了网络结构并结合了其他人的多项工作后，训练出了超过100层的网络模型，ResNet。这也是当年ImageNet的获胜模型，top5识别率提升到96%左右。ResNet中引入的隔层连接显著改善了神经网络的训练难的问题，到目前（2017年末）ResNet仍然是CNN研究和应用的首选基准模型。本文的后续实验也都基于该模型。

#### 图像分类问题

图像分类是给一副图像分配一个或多个标签，指出图像描绘了什么场景、包含了什么物体等。只有进行了这样的转换，计算机才可以一定程度上理解图像，否则它只是一个没有任何语义信息的矩阵。图像分类是计算机视觉的核心问题，因为计算机视觉领域的大多数其他问题都可以被分解为分类问题，比如物体识别是在图像的多个位置进行分类，图像分割是在像素级别上分类。然而，这个基础问题又非常困难。图像的生成过程受很多因素影响，可以将其分为环境条件和自身变化两类。前者包括光照、拍摄角度和遮挡，后者的情况更加复杂，以人脸为例，个体的发型、饰物和表情变化都将显著改变图像矩阵的取值。这些因素的自由度都太大，对其正向建模几乎不可能。因此，以数据驱动位思想的机器学习方法更合适。卷积神经网络就是为分析图像而设计的一种机器学习模型，近些年成为图像分类的标准方法。本文也以在图像分类问题上的效果来衡量正则化方法对CNN的有效性。

##### 数据集

MNIST

CIFAR-10是一个很流行的图像分类测试数据集。它包含的是3通道的32，32尺寸的图像，类别包含了猫、狗等动物和轿车、轮船等交通工具共十个。图像总量是6万张，被划分为5万和1万的两个子集，一般分别被当做训练集和测试集。另有CIFAR-100数据集，包含的图像和CIFAR-10完全一致，不过类别的粒度更细，比如人被分为了婴儿、男孩和女孩等，总计有100个类别。

ImageNet

#### 过拟合与正则化

##### VC维

##### bias-variance困境

### 显式正则化

#### 参数衰减

#### 噪声引入

##### 随机梯度下降

##### Dropout

##### Batch Normalization

### 数据扩增

#### 仿射变换

#### 图像增强

### 特征扩增

#### 特征扩增层

#### 扩增操作

#### 实验

#### 结论

### 组合实验

### 总结



#### 



